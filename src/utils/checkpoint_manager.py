# Class generated by Cursor with minor modifications

import json
from datetime import datetime
from pathlib import Path

import torch
import torch.nn as nn
import yaml


class CheckpointManager:
    def __init__(self, save_dir: str, model_name: str, config: dict):
        """
        Initialize checkpoint manager.

        Args:
            save_dir: Directory to save checkpoints
            model_name: Name of the model (e.g., 'autoencoder')
            config: Configuration dictionary used for training
        """
        self.save_dir = Path(save_dir)
        self.model_name = model_name
        self.config = config
        self.best_val_loss = float("inf")

        # Create save directory if it doesn't exist
        self.save_dir.mkdir(parents=True, exist_ok=True)

        # Save config file immediately
        self._save_config()

    def _save_config(self):
        """Save configuration to the checkpoint directory."""
        config_path = self.save_dir / "config.yaml"
        with open(config_path, "w") as f:
            yaml.dump(self.config, f, default_flow_style=False)

    def save_checkpoint(
        self,
        model: nn.Module,
        optimizer: torch.optim.Optimizer,
        epoch: int,
        val_loss: float,
        train_loss: float,
        is_best: bool | None = None,
    ):
        """
        Save model checkpoint.

        Args:
            model: PyTorch model
            optimizer: PyTorch optimizer
            epoch: Current epoch
            val_loss: Validation loss
            train_loss: Training loss
            is_best: Optional override for whether this is the best model
        """
        is_best = is_best if is_best is not None else val_loss < self.best_val_loss

        # Create checkpoint dictionary
        checkpoint = {
            "epoch": epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "val_loss": val_loss,
            "train_loss": train_loss,
            "config": self.config,  # Include config in checkpoint
            "timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
        }

        # Save the checkpoint
        if is_best:
            self.best_val_loss = val_loss

            # Remove previous best model if it exists
            best_path = self.save_dir / f"{self.model_name}_best.pth"
            if best_path.exists():
                best_path.unlink()

            # Save new best model
            torch.save(checkpoint, best_path)

            # Save a metadata file with training info
            metadata = {
                "best_val_loss": float(self.best_val_loss),
                "epoch": epoch,
                "timestamp": checkpoint["timestamp"],
                "train_loss": float(train_loss),
            }

            metadata_path = self.save_dir / "best_model_metadata.json"
            with open(metadata_path, "w") as f:
                json.dump(metadata, f, indent=4)

    @staticmethod
    def load_checkpoint(checkpoint_path: str | Path, model, optimizer=None):
        """
        Load model from checkpoint.

        Args:
            checkpoint_path: Path to the checkpoint
            model: PyTorch model to load weights into
            optimizer: Optional optimizer to load state into

        Returns:
            model: Loaded model
            optimizer: Loaded optimizer (if provided)
            config: Configuration used for training
        """
        if isinstance(checkpoint_path, str):
            checkpoint_path = Path(checkpoint_path)

        if not checkpoint_path.exists():
            raise FileNotFoundError(f"No checkpoint found at {checkpoint_path}")

        checkpoint = torch.load(checkpoint_path)
        model.load_state_dict(checkpoint["model_state_dict"])
        if optimizer:
            optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

        return model, optimizer
